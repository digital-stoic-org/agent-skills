# OpenSpec Test Reference

**Note**: This reference documents framework-specific testing patterns for **future specialized skills** (test-pytest, test-jest, test-cargo). The main openspec-test skill is now a generic test.md executor and does not use these patterns directly.

## Framework Detection

Detect test framework from project files:

```yaml
javascript:
  files: [package.json]
  frameworks:
    jest: '"jest"' in dependencies/devDependencies
    vitest: '"vitest"' in dependencies/devDependencies
    mocha: '"mocha"' in dependencies/devDependencies
  default: "npm test"

python:
  files: [pyproject.toml, pytest.ini, setup.py]
  frameworks:
    pytest: pytest.ini exists OR "pytest" in pyproject.toml
    unittest: test_*.py pattern without pytest
  default: "pytest"

rust:
  files: [Cargo.toml]
  default: "cargo test"

go:
  files: [go.mod]
  default: "go test ./..."
```

**Fallback**: If no framework detected, prompt for test command:
```
âš ï¸ No test framework detected
Project has: {detected config files}

What command runs tests? (or 'skip' to skip automated tests)
```

## Test Layers

```yaml
layers:
  smoke:
    purpose: Basic health checks
    examples: ["app starts", "endpoints respond", "no crash on basic input"]
    auto: true

  integration:
    purpose: Component interactions
    examples: ["API contracts match", "DB state consistent", "error handling"]
    auto: true

  manual:
    purpose: Human-verified critical paths
    examples: ["UI flows", "user journeys", "edge cases needing judgment"]
    auto: false  # Generate instructions only

  pbt:
    purpose: Property-based edge case discovery
    examples: ["invariants hold", "no unexpected states", "boundary conditions"]
    auto: true
    frameworks: [hypothesis, fast-check, proptest]
```

## Test Command Patterns

When running tests, use framework-appropriate commands:

```bash
# Smoke (fast, basic)
npm test -- --grep "smoke"
pytest -m smoke
cargo test --lib

# Integration (slower, thorough)
npm test -- --grep "integration"
pytest -m integration
cargo test --test '*'

# PBT (property-based)
npm test -- --grep "property"
pytest -m "property or hypothesis"
cargo test --features proptest
```

Adapt patterns based on detected framework and existing test structure.

## Manual Test Instructions Template

For manual layer, generate human-executable steps:

```markdown
## Manual Test Instructions: {change-id}

### Test 1: {test name}
**Purpose**: {what this validates}
**Steps**:
1. {step 1}
2. {step 2}
3. {step 3}

**Expected**: {expected outcome}
**Pass criteria**: {what constitutes pass}

---

### Test 2: {test name}
...

---

When complete, report results:
- Test 1: PASS / FAIL (reason)
- Test 2: PASS / FAIL (reason)
```

## Output Formats

### test command
```
ğŸ§ª Test Results: {change-id}

Layer      | Status | Details
-----------|--------|--------
Smoke      | âœ…/âŒ   | {count} tests
Integration| âœ…/âŒ   | {count} tests
Manual     | ğŸ“‹     | {n} instructions generated
PBT        | âœ…/â­ï¸   | {count} tests or "skipped"

Overall: âœ… Ready for gate / âŒ Fix required
```

### layer command
```
ğŸ§ª Layer: {layer-name} for {change-id}

Status: âœ…/âŒ
Tests: {passed}/{total}
Details: {summary}
```

### status command (no history)
```
ğŸ§ª Test Status: {change-id}

No test runs recorded. Run: /openspec-test test {change-id}
```

### status command (with history)
```
ğŸ§ª Test Status: {change-id}

Layer      | Last Run   | Status
-----------|------------|-------
Smoke      | {datetime} | âœ…/âŒ
Integration| {datetime} | âœ…/âŒ
Manual     | pending    | ğŸ“‹ Awaiting human
PBT        | {datetime} | âœ…/â­ï¸
```

### checkpoint PASS
```
âœ… GATE {n}: {description} [PASS]
Smoke: {passed}/{total} tests passed
â†’ Continue: /openspec-develop section {change-id} {n+1}
```

### checkpoint PARTIAL
```
âš ï¸ GATE {n}: {description} [PARTIAL]
Smoke: {passed}/{total} tests passed
Failures:
- {test}: {reason}
â†’ Fix and re-run: /openspec-test checkpoint {change-id} {n}
```

### checkpoint BLOCKED
```
ğŸš« GATE {n}: {description} [BLOCKED]
Reason: {blocker description}
Options:
â†’ Fix blocker and retry: /openspec-test checkpoint {change-id} {n}
â†’ Skip gate: /openspec-develop section {change-id} {n+1}
â†’ Replan: /openspec-replan {change-id}
```

## Log Formats

### Test Log Format

Write human summary to `openspec/changes/{change-id}/test-logs/gate-{n}.md`:

```markdown
# Test Log: GATE {n} â€” {description}

**Run**: {ISO 8601 timestamp}
**Mode**: {mode} ({layers})
**Result**: âœ… PASS / âš ï¸ PARTIAL / âŒ FAIL

## Summary

| Step | Type | Status | Duration |
|------|------|--------|----------|
| {id} | auto | âœ…/âŒ | {Xs} |
| {id} | smoke | âœ…/ğŸ“‹ | {Xs} |
| {id} | manual | âœ…/âŒ/ğŸ“‹ | â€” |

**Auto**: {pass}/{total} âœ… | **Smoke**: {pass}/{total} ğŸ“‹ | **Manual**: {pass}/{total} ğŸ‘¤
**Total duration**: {Xs}

---

## {step-id} {description} [{type}]

- **Command**: `{exact command run}`
- **Expected**: {pass criteria from test.md}
- **Stdout**: {first 50 lines, or "(empty)"}
- **Stderr**: {first 20 lines, or "(none)"}
- **Exit code**: {code}
- **Duration**: {Xs}
- **Result**: âœ… PASS / âŒ FAIL â€” {1-line reason}

---

## Next Action

{If PASS}: â†’ Continue: `/openspec-develop section {change-id} {n+1}`
{If PARTIAL}: â†’ Fix failures, re-run: `/openspec-test checkpoint {change-id} {n}`
{If BLOCKED}: â†’ Resolve blocker: {specific instruction}
```

On re-run: append new timestamped entry (separated by `---\n\n`), don't overwrite.

### Raw Log Format

Write raw captures to `openspec/changes/{change-id}/test-logs/gate-{n}-raw.json`:

```json
{
  "gate": 0,
  "change_id": "{change-id}",
  "runs": [
    {
      "timestamp": "ISO 8601",
      "mode": "garage",
      "result": "PASS",
      "steps": [
        {
          "id": "0.5",
          "description": "step description",
          "type": "auto|smoke|manual",
          "command": "exact command run",
          "expected": "pass criteria from test.md",
          "stdout": "full output (mask secrets)",
          "stderr": "",
          "exit_code": 0,
          "duration_s": 0,
          "result": "PASS|FAIL",
          "reason": null
        }
      ],
      "summary": {
        "auto": {"pass": 0, "total": 0},
        "smoke": {"pass": 0, "total": 0},
        "manual": {"pass": 0, "total": 0},
        "total_duration_s": 0
      }
    }
  ]
}
```

**Rules**:
- Write each step immediately after execution
- Full stdout/stderr â€” no truncation (except mask API keys/tokens: first 7 + last 4 chars)
- On re-run: append to `runs[]` array, never overwrite
- Markdown `gate-{n}.md` is rendered from this JSON â€” single source of truth

## Test Progression Strategy

Mandatory first: smoke tests (basic sanity, < 5s). Then risk-based progression:

1. **Smoke** â€” app starts, no crashes (mandatory)
2. **Unit** â€” functions in isolation (< 10s)
3. **Integration** â€” components interact (< 30s)
4. **E2E** â€” full user flows (> 30s)

Garage mode can stop after smoke. Scale mode runs all. test.md MUST start with smoke tests, ordered fastâ†’slow.

## Execution Tracing

### Per-Step Capture Protocol

For every `[auto]` and `[smoke]` step, capture:

```yaml
capture:
  command: "{exact command from test.md}"
  expected: "{pass criteria from test.md}"
  stdout: "{raw output, truncated per rules}"
  stderr: "{raw stderr, truncated per rules}"
  exit_code: 0
  duration_s: 2.3
  result: "PASS"  # or "FAIL â€” {reason}"
```

**Truncation rules**:
- stdout >50 lines â†’ first 30 + `\n... ({N} lines omitted)\n` + last 10
- stderr >20 lines â†’ first 15 + `\n... ({N} lines omitted)\n`
- Single lines >500 chars â†’ first 200 + `...` + last 50
- Sensitive values (API keys, tokens) â†’ mask: `sk-ant-...xxxx` (first 7 + last 4)

### Failure Diagnostics

When a step fails, include diagnostic context:

```markdown
- **Result**: âŒ FAIL â€” exit code 1 (expected 0)
- **Diagnostic**: stderr shows "Read-only file system" â€” mount flag :ro is applied but command expected write access
- **Fix hint**: Check if test expectation matches. Step expects non-zero exit for read-only validation.
```

**Diagnostic heuristic**:
- Exit code mismatch â†’ report expected vs actual
- Stderr contains error â†’ extract first error line
- Timeout â†’ report command duration vs expected
- Empty stdout when output expected â†’ flag as "no output produced"

## Error Handling

### Test failure
```
âŒ Layer failed: {layer-name}

Failed tests:
- {test 1}: {reason}
- {test 2}: {reason}

Fix the failures before proceeding. Re-run: /openspec-test layer {change-id} {layer}
```

### No tests found
```
âš ï¸ No tests found for layer: {layer-name}

This could mean:
1. Tests not yet written for this change
2. Test naming convention doesn't match expected pattern

Options:
- Write tests matching convention
- Skip layer with: /openspec-test layer {change-id} {next-layer}
```

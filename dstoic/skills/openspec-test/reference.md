# OpenSpec Test Reference

**Note**: This reference documents framework-specific testing patterns for **future specialized skills** (test-pytest, test-jest, test-cargo). The main openspec-test skill is now a generic test.md executor and does not use these patterns directly.

## Framework Detection

Detect test framework from project files:

```yaml
javascript:
  files: [package.json]
  frameworks:
    jest: '"jest"' in dependencies/devDependencies
    vitest: '"vitest"' in dependencies/devDependencies
    mocha: '"mocha"' in dependencies/devDependencies
  default: "npm test"

python:
  files: [pyproject.toml, pytest.ini, setup.py]
  frameworks:
    pytest: pytest.ini exists OR "pytest" in pyproject.toml
    unittest: test_*.py pattern without pytest
  default: "pytest"

rust:
  files: [Cargo.toml]
  default: "cargo test"

go:
  files: [go.mod]
  default: "go test ./..."
```

**Fallback**: If no framework detected, prompt for test command:
```
âš ï¸ No test framework detected
Project has: {detected config files}

What command runs tests? (or 'skip' to skip automated tests)
```

## Test Layers

```yaml
layers:
  smoke:
    purpose: Basic health checks
    examples: ["app starts", "endpoints respond", "no crash on basic input"]
    auto: true

  integration:
    purpose: Component interactions
    examples: ["API contracts match", "DB state consistent", "error handling"]
    auto: true

  manual:
    purpose: Human-verified critical paths
    examples: ["UI flows", "user journeys", "edge cases needing judgment"]
    auto: false  # Generate instructions only

  pbt:
    purpose: Property-based edge case discovery
    examples: ["invariants hold", "no unexpected states", "boundary conditions"]
    auto: true
    frameworks: [hypothesis, fast-check, proptest]
```

## Test Command Patterns

When running tests, use framework-appropriate commands:

```bash
# Smoke (fast, basic)
npm test -- --grep "smoke"
pytest -m smoke
cargo test --lib

# Integration (slower, thorough)
npm test -- --grep "integration"
pytest -m integration
cargo test --test '*'

# PBT (property-based)
npm test -- --grep "property"
pytest -m "property or hypothesis"
cargo test --features proptest
```

Adapt patterns based on detected framework and existing test structure.

## Manual Test Instructions Template

For manual layer, generate human-executable steps:

```markdown
## Manual Test Instructions: {change-id}

### Test 1: {test name}
**Purpose**: {what this validates}
**Steps**:
1. {step 1}
2. {step 2}
3. {step 3}

**Expected**: {expected outcome}
**Pass criteria**: {what constitutes pass}

---

### Test 2: {test name}
...

---

When complete, report results:
- Test 1: PASS / FAIL (reason)
- Test 2: PASS / FAIL (reason)
```

## Output Formats

### test command
```
ğŸ§ª Test Results: {change-id}

Layer      | Status | Details
-----------|--------|--------
Smoke      | âœ…/âŒ   | {count} tests
Integration| âœ…/âŒ   | {count} tests
Manual     | ğŸ“‹     | {n} instructions generated
PBT        | âœ…/â­ï¸   | {count} tests or "skipped"

Overall: âœ… Ready for gate / âŒ Fix required
```

### layer command
```
ğŸ§ª Layer: {layer-name} for {change-id}

Status: âœ…/âŒ
Tests: {passed}/{total}
Details: {summary}
```

### status command (no history)
```
ğŸ§ª Test Status: {change-id}

No test runs recorded. Run: /openspec-test test {change-id}
```

### status command (with history)
```
ğŸ§ª Test Status: {change-id}

Layer      | Last Run   | Status
-----------|------------|-------
Smoke      | {datetime} | âœ…/âŒ
Integration| {datetime} | âœ…/âŒ
Manual     | pending    | ğŸ“‹ Awaiting human
PBT        | {datetime} | âœ…/â­ï¸
```

### checkpoint PASS
```
âœ… GATE {n}: {description} [PASS]
Smoke: {passed}/{total} tests passed
â†’ Continue: /openspec-develop section {change-id} {n+1}
```

### checkpoint PARTIAL
```
âš ï¸ GATE {n}: {description} [PARTIAL]
Smoke: {passed}/{total} tests passed
Failures:
- {test}: {reason}
â†’ Fix and re-run: /openspec-test checkpoint {change-id} {n}
```

### checkpoint BLOCKED
```
ğŸš« GATE {n}: {description} [BLOCKED]
Reason: {blocker description}
Options:
â†’ Fix blocker and retry: /openspec-test checkpoint {change-id} {n}
â†’ Skip gate: /openspec-develop section {change-id} {n+1}
â†’ Replan: /openspec-replan {change-id}
```

## Execution Tracing

### Per-Step Capture Protocol

For every `[auto]` and `[smoke]` step, capture:

```yaml
capture:
  command: "{exact command from test.md}"
  expected: "{pass criteria from test.md}"
  stdout: "{raw output, truncated per rules}"
  stderr: "{raw stderr, truncated per rules}"
  exit_code: 0
  duration_s: 2.3
  result: "PASS"  # or "FAIL â€” {reason}"
```

**Truncation rules**:
- stdout >50 lines â†’ first 30 + `\n... ({N} lines omitted)\n` + last 10
- stderr >20 lines â†’ first 15 + `\n... ({N} lines omitted)\n`
- Single lines >500 chars â†’ first 200 + `...` + last 50
- Sensitive values (API keys, tokens) â†’ mask: `sk-ant-...xxxx` (first 7 + last 4)

### Failure Diagnostics

When a step fails, include diagnostic context:

```markdown
- **Result**: âŒ FAIL â€” exit code 1 (expected 0)
- **Diagnostic**: stderr shows "Read-only file system" â€” mount flag :ro is applied but command expected write access
- **Fix hint**: Check if test expectation matches. Step expects non-zero exit for read-only validation.
```

**Diagnostic heuristic**:
- Exit code mismatch â†’ report expected vs actual
- Stderr contains error â†’ extract first error line
- Timeout â†’ report command duration vs expected
- Empty stdout when output expected â†’ flag as "no output produced"

## Error Handling

### Test failure
```
âŒ Layer failed: {layer-name}

Failed tests:
- {test 1}: {reason}
- {test 2}: {reason}

Fix the failures before proceeding. Re-run: /openspec-test layer {change-id} {layer}
```

### No tests found
```
âš ï¸ No tests found for layer: {layer-name}

This could mean:
1. Tests not yet written for this change
2. Test naming convention doesn't match expected pattern

Options:
- Write tests matching convention
- Skip layer with: /openspec-test layer {change-id} {next-layer}
```
